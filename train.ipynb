{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1406,
     "status": "ok",
     "timestamp": 1680546922536,
     "user": {
      "displayName": "Yuxiao Liu",
      "userId": "00349421896740538734"
     },
     "user_tz": 240
    },
    "id": "QQsK0kcjOK8F",
    "outputId": "70cf75e3-fc67-42d0-cc54-a2ac046bcd50"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/MyDrive/si699-music-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6UgeAtwKJ8Q"
   },
   "outputs": [],
   "source": [
    "!python3 preprocessing/convert_npy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gB3UgwVXJoY4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "import glob\n",
    "import torchaudio\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import csv\n",
    "from transformers import AutoConfig, AutoFeatureExtractor, Wav2Vec2FeatureExtractor\n",
    "from sklearn.metrics import *\n",
    "# from run.models import *\n",
    "from run.attention_modules import *\n",
    "import collections\n",
    "import torch\n",
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.functional.classification import multilabel_auroc\n",
    "from torchmetrics.classification import MultilabelPrecision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "torch.manual_seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "print(\"Run on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"data/autotagging_moodtheme/0*/*.mp3\"\n",
    "files = sorted(glob.glob(data_root))\n",
    "print(\"Size:\", len(files))\n",
    "waveform, sr = librosa.load(files[0], sr=None, mono=True, offset=0.0, duration=None)\n",
    "plt.figure(figsize=(15, 3))\n",
    "display.waveshow(y=waveform, sr=sr)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=waveform, sr=sr)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(3)\n",
    "fig.set_figwidth(18)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                         y_axis='mel', sr=sr,\n",
    "                         fmax=8000, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6465,
     "status": "ok",
     "timestamp": 1680546932702,
     "user": {
      "displayName": "Yuxiao Liu",
      "userId": "00349421896740538734"
     },
     "user_tz": 240
    },
    "id": "lrWxObbsPRG-",
    "outputId": "1ef1171a-b975-4490-c24d-6118f2ea9b54"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tracks_dict, npy_root, config, tags, data_type, feature_extractor_type):\n",
    "        self.npy_root = npy_root\n",
    "        self.config = config\n",
    "        self.tracks_dict = tracks_dict\n",
    "        self.tags = tags\n",
    "        self.mlb = LabelBinarizer().fit(self.tags)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.title_dict = {}\n",
    "        self.prepare_title()\n",
    "        self.data = []\n",
    "        self.input_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.labels = []\n",
    "        self.data_type = data_type\n",
    "        self.prepare_data()\n",
    "        self.feature_extractor_type = feature_extractor_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert 0 <= index < len(self)\n",
    "        waveform = self.data[index]\n",
    "        input_ids = self.input_ids[index]\n",
    "        attention_mask = self.attention_mask[index]\n",
    "        target = self.labels[index]\n",
    "        if self.feature_extractor_type == 'raw':\n",
    "            mel_spec = torch.Tensor(waveform)\n",
    "        if self.feature_extractor_type == 'ast':\n",
    "            feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "                \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
    "                sampling_rate=self.config['sample_rate'],\n",
    "                num_mel_bins=self.config['n_mels']\n",
    "            )\n",
    "            encoding = feature_extractor(waveform, sampling_rate=self.config['sample_rate'], annotations=target, return_tensors=\"pt\")\n",
    "            mel_spec = encoding['input_values'].squeeze()\n",
    "            mel_spec = torch.transpose(mel_spec, 0, 1)\n",
    "        if self.feature_extractor_type == 'wav2vec':\n",
    "            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "                \"facebook/wav2vec2-base-960h\"\n",
    "            )\n",
    "            encoding = feature_extractor(waveform, sampling_rate=self.config['sample_rate'],\n",
    "                                         return_tensors=\"pt\")\n",
    "            mel_spec = encoding['input_values'].squeeze()\n",
    "        return mel_spec, input_ids, attention_mask, target\n",
    "    \n",
    "    def prepare_title(self):\n",
    "        whole_filenames = sorted(glob.glob(os.path.join(self.npy_root, \"*/*.npy\")))\n",
    "        titles = []\n",
    "        for filename in whole_filenames:\n",
    "            file_id = os.path.join(filename.split('/')[-2], filename.split('/')[-1].split('.')[0])\n",
    "            titles.append(self.tracks_dict[file_id][1])\n",
    "        encoding = self.tokenizer(titles, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        for idx, filename in enumerate(whole_filenames):\n",
    "            file_id = os.path.join(filename.split('/')[-2], filename.split('/')[-1].split('.')[0])\n",
    "            self.title_dict[file_id] = (input_ids[idx], attention_mask[idx])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        whole_filenames = sorted(glob.glob(os.path.join(self.npy_root, \"*/*.npy\")))\n",
    "        train_size = int(len(whole_filenames) * 0.8)\n",
    "        filenames = []\n",
    "        random.shuffle(whole_filenames)\n",
    "        if self.data_type == 'train':\n",
    "            filenames = whole_filenames[:train_size]\n",
    "        if self.data_type == 'valid':\n",
    "            filenames = whole_filenames[train_size:]\n",
    "        for filename in tqdm(filenames):\n",
    "            file_id = os.path.join(filename.split('/')[-2], filename.split('/')[-1].split('.')[0])\n",
    "            if file_id not in self.tracks_dict:\n",
    "                print(file_id)\n",
    "                continue\n",
    "            self.data.append(np.load(filename))\n",
    "            self.input_ids.append(self.title_dict[file_id][0])\n",
    "            self.attention_mask.append(self.title_dict[file_id][1])\n",
    "            self.labels.append(np.sum(self.mlb.transform(self.tracks_dict[file_id][0]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(tag_file, npy_root, isMap):\n",
    "    id2title_dict = {}\n",
    "    with open('data/raw.meta.tsv') as fp:\n",
    "        reader = csv.reader(fp, delimiter='\\t')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            id2title_dict[row[0]] = row[3]\n",
    "\n",
    "    if isMap:\n",
    "        f = open('tag_categorize.json')\n",
    "        data = json.load(f)\n",
    "        categorize = {}\n",
    "        for k, v in data.items():\n",
    "            for i in v[1:-1].split(', '):\n",
    "                categorize[i] = k\n",
    "    tracks = {}\n",
    "    total_tags = []\n",
    "    with open(tag_file) as fp:\n",
    "        reader = csv.reader(fp, delimiter='\\t')\n",
    "        next(reader, None)  # skip header\n",
    "        for row in reader:\n",
    "            if not os.path.exists(os.path.join(npy_root, row[3].replace('.mp3', '.npy'))):\n",
    "                print(os.path.join(npy_root, row[3].replace('.mp3', '.npy')))\n",
    "                continue\n",
    "            track_id = row[3].split('.')[0]\n",
    "            tags = []\n",
    "            for tag in row[5:]:\n",
    "                if isMap:\n",
    "                    tags.append(categorize[tag.split('---')[-1]])\n",
    "                else:\n",
    "                    tags.append(tag.split('---')[-1])\n",
    "            tracks[track_id] = (list(set(tags)), id2title_dict[row[0]])\n",
    "            total_tags += list(set(tags))\n",
    "    print(\"Distribution of tags:\", collections.Counter(total_tags))\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.hist(total_tags)\n",
    "    plt.savefig('dist.png')\n",
    "    return tracks, list(set(total_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "tag_file = 'data/autotagging_moodtheme.tsv'\n",
    "npy_root = 'data/waveform'\n",
    "tracks_dict, tags = get_tags(tag_file, npy_root, True)\n",
    "N_CLASSES = len(tags)\n",
    "print(N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = 'raw'\n",
    "batch_size = 4\n",
    "train_dataset = MyDataset(tracks_dict, npy_root, config, tags, \"train\", transform)\n",
    "val_dataset = MyDataset(tracks_dict, npy_root, config, tags, \"valid\", transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for waveform, input_ids, attention_mask, label in val_loader:\n",
    "    print(waveform)\n",
    "    print(input_ids)\n",
    "    print(attention_mask)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfVtoBLpK7Us"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, criterion, optimizer, train_loader, is_title=False):\n",
    "    losses = []\n",
    "    ground_truth = []\n",
    "    prediction = []\n",
    "    model.train()\n",
    "    for waveform, input_ids, attention_mask, label in tqdm(train_loader):\n",
    "        waveform, label = waveform.to(device), label.to(device)\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        if is_title:\n",
    "            output = model(waveform, input_ids, attention_mask)\n",
    "        else:\n",
    "            output = model(waveform)\n",
    "        loss = criterion(output, label.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.cpu().detach())\n",
    "        ground_truth.append(label)\n",
    "        prediction.append(output)\n",
    "    get_eval_metrics(prediction, ground_truth, 'train', epoch, losses)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, epoch, criterion, val_loader, is_title=False):\n",
    "    losses = []\n",
    "    ground_truth = []\n",
    "    prediction = []\n",
    "    model.eval()\n",
    "    for waveform, input_ids, attention_mask, label in tqdm(val_loader):\n",
    "        waveform, label = waveform.to(device), label.to(device)\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        if is_title:\n",
    "            output = model(waveform, input_ids, attention_mask)\n",
    "        else:\n",
    "            output = model(waveform)\n",
    "        loss = criterion(output, label.float())\n",
    "        losses.append(loss.cpu().detach())\n",
    "        ground_truth.append(label)\n",
    "        prediction.append(output)\n",
    "    pre = get_eval_metrics(prediction, ground_truth, 'val', epoch, losses)\n",
    "    return pre\n",
    "\n",
    "\n",
    "def get_eval_metrics(outputs, labels, run_type, epoch, losses):\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    assert outputs.shape == labels.shape\n",
    "    # 1. number of correctly predicted tags divided by the total number of tags\n",
    "    prob_classes = []\n",
    "    for i in range(labels.size(0)):\n",
    "        label = labels[i]\n",
    "        k = label.sum()\n",
    "        _, idx = outputs[i].topk(k=k)\n",
    "        predict = torch.zeros_like(outputs[i])\n",
    "        predict[idx] = 1\n",
    "        prob_classes.append(predict)\n",
    "    prob_classes = torch.stack(prob_classes)\n",
    "    matched_1s = torch.mul(prob_classes, labels)\n",
    "    correct_tag_percentage = matched_1s.sum() / labels.sum()\n",
    "\n",
    "    # 2. Auroc\n",
    "    auroc = multilabel_auroc(outputs, labels, num_labels=N_CLASSES, average=\"macro\", thresholds=None).item()\n",
    "\n",
    "    # 3. avg precision\n",
    "    metric = MultilabelPrecision(average='macro', num_labels=N_CLASSES, thresholds=None).to(device)\n",
    "    pre = metric(outputs, labels).item()\n",
    "\n",
    "    # write tensorboard and logging file\n",
    "    writer.add_scalar(\"Loss/{}\".format(run_type), np.mean(losses), epoch)\n",
    "    writer.add_scalar(\"Auroc/{}\".format(run_type), auroc, epoch)\n",
    "    writer.add_scalar(\"Pre/{}\".format(run_type), pre, epoch)\n",
    "    writer.add_scalar(\"Avg_percent/{}\".format(run_type), correct_tag_percentage, epoch)\n",
    "    print(\"{} - epoch: {}, loss: {}, auroc: {}, pre: {}, avg percent: {}\".format(\n",
    "        run_type, epoch, np.mean(losses), auroc, pre, correct_tag_percentage))\n",
    "    logging.info(\"{} - epoch: {}, loss: {}, auroc: {}, pre: {}, avg percent: {}\".format(\n",
    "        run_type, epoch, np.mean(losses), auroc, pre, correct_tag_percentage))\n",
    "    return correct_tag_percentage\n",
    "\n",
    "\n",
    "def get_model(model_name, tags):\n",
    "    if model_name =='samplecnn':\n",
    "        model = SampleCNN(N_CLASSES, config).to(device)\n",
    "    elif model_name == 'crnn':\n",
    "        model = CRNN(N_CLASSES, config).to(device)\n",
    "    elif model_name =='fcn':\n",
    "        model = FCN(N_CLASSES, config).to(device)\n",
    "    elif model_name == 'musicnn':\n",
    "        model = Musicnn(N_CLASSES, config).to(device)\n",
    "    elif model_name == 'musicnn_title':\n",
    "        model = MusicnnwithTitle(N_CLASSES, config).to(device)\n",
    "    elif model_name == 'shortchunkcnn_res':\n",
    "        model = ShortChunkCNN_Res(N_CLASSES, config).to(device)\n",
    "    elif model_name == 'cnnsa':\n",
    "        model = CNNSA(N_CLASSES, config).to(device)\n",
    "    elif model_name == 'baseline2':\n",
    "        model = Baseline2(N_CLASSES, config).to(device)\n",
    "    elif model_name == 'wav2vec':\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            \"facebook/wav2vec2-base-960h\",\n",
    "            num_labels=N_CLASSES,\n",
    "            label2id={label: i for i, label in enumerate(tags)},\n",
    "            id2label={i: label for i, label in enumerate(tags)},\n",
    "            finetuning_task=\"wav2vec2_clf\",\n",
    "        )\n",
    "        model = Wav2Vec2ForSpeechClassification(model_config).to(device)\n",
    "    else:\n",
    "        model = SampleCNN(N_CLASSES, config).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicnnwithTitle(nn.Module):\n",
    "    def __init__(self, num_classes, config=None):\n",
    "        super(Musicnn, self).__init__()\n",
    "        self.spec = torchaudio.transforms.MelSpectrogram(sample_rate=config['sample_rate'],\n",
    "                                                  n_fft=config['n_fft'],\n",
    "                                                  f_min=config['fmin'],\n",
    "                                                  f_max=config['fmax'],\n",
    "                                                  n_mels=config['n_mels'])\n",
    "\n",
    "        # Spectrogram\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.spec_bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        # Pons front-end\n",
    "        m1 = Conv_V(1, 204, (int(0.7 * 96), 7))\n",
    "        m2 = Conv_V(1, 204, (int(0.4 * 96), 7))\n",
    "        m3 = Conv_H(1, 51, 129)\n",
    "        m4 = Conv_H(1, 51, 65)\n",
    "        m5 = Conv_H(1, 51, 33)\n",
    "        self.layers = nn.ModuleList([m1, m2, m3, m4, m5])\n",
    "\n",
    "        # Pons back-end\n",
    "        backend_channel = 512\n",
    "        self.layer1 = Conv_1d(561, backend_channel, kernel_size=7, stride=1, padding=3, pooling=1)\n",
    "        self.layer2 = Conv_1d(backend_channel, backend_channel, kernel_size=7, stride=1, padding=3, pooling=1)\n",
    "        self.layer3 = Conv_1d(backend_channel, backend_channel, kernel_size=7, stride=1, padding=3, pooling=1)\n",
    "\n",
    "        # Dense\n",
    "        dense_channel = 500\n",
    "        self.dense1 = nn.Linear((561 + (backend_channel * 3)) * 2, dense_channel)\n",
    "        self.bn = nn.BatchNorm1d(dense_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(dense_channel, num_classes)\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dense3 = nn.Linear(self.bert.config.hidden_size, 256)\n",
    "        self.dense4 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self.dense5 = nn.Linear(2*num_classes, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x, input_ids=None, attention_mask=None):\n",
    "        # Spectrogram\n",
    "        x = self.spec(x)\n",
    "        x = self.to_db(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.spec_bn(x)\n",
    "\n",
    "        # Pons front-end\n",
    "        out = []\n",
    "        for layer in self.layers:\n",
    "            out.append(layer(x))\n",
    "        out = torch.cat(out, dim=1)\n",
    "\n",
    "        # Pons back-end\n",
    "        length = out.size(2)\n",
    "        res1 = self.layer1(out)\n",
    "        res2 = self.layer2(res1) + res1\n",
    "        res3 = self.layer3(res2) + res2\n",
    "        out = torch.cat([out, res1, res2, res3], 1)\n",
    "\n",
    "        mp = nn.MaxPool1d(length)(out)\n",
    "        avgp = nn.AvgPool1d(length)(out)\n",
    "\n",
    "        out = torch.cat([mp, avgp], dim=1)\n",
    "        out = out.squeeze(2)\n",
    "\n",
    "        out = self.relu(self.bn(self.dense1(out)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense2(out)\n",
    "        \n",
    "        out_title = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        out_title = self.dense3(out_title.pooler_output)\n",
    "        out_title = self.dropout(out_title)\n",
    "        out_title = self.dense4(out_title)\n",
    "        \n",
    "        out = torch.cat((out_title, out), dim=1)\n",
    "        out = self.dense5(out)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "class Res_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, stride=2):\n",
    "        super(Res_2d, self).__init__()\n",
    "        # convolution\n",
    "        self.conv_1 = nn.Conv2d(input_channels, output_channels, shape, stride=stride, padding=shape//2)\n",
    "        self.bn_1 = nn.BatchNorm2d(output_channels)\n",
    "        self.conv_2 = nn.Conv2d(output_channels, output_channels, shape, padding=shape//2)\n",
    "        self.bn_2 = nn.BatchNorm2d(output_channels)\n",
    "\n",
    "        # residual\n",
    "        self.diff = False\n",
    "        if (stride != 1) or (input_channels != output_channels):\n",
    "            self.conv_3 = nn.Conv2d(input_channels, output_channels, shape, stride=stride, padding=shape//2)\n",
    "            self.bn_3 = nn.BatchNorm2d(output_channels)\n",
    "            self.diff = True\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolution\n",
    "        out = self.bn_2(self.conv_2(self.relu(self.bn_1(self.conv_1(x)))))\n",
    "\n",
    "        # residual\n",
    "        if self.diff:\n",
    "            x = self.bn_3(self.conv_3(x))\n",
    "        out = x + out\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class CNNSA(nn.Module):\n",
    "    '''\n",
    "    Won et al. 2019\n",
    "    Toward interpretable music tagging with self-attention.\n",
    "    Feature extraction with CNN + temporal summary with Transformer encoder.\n",
    "    '''\n",
    "    def __init__(self, n_class, config=None):\n",
    "        super(CNNSA, self).__init__()\n",
    "        self.spec = torchaudio.transforms.MelSpectrogram(sample_rate=config['sample_rate'],\n",
    "                                                  n_fft=config['n_fft'],\n",
    "                                                  f_min=config['fmin'],\n",
    "                                                  f_max=config['fmax'],\n",
    "                                                  n_mels=config['n_mels'])\n",
    "\n",
    "        # Spectrogram\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.spec_bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        # CNN\n",
    "        n_channels = 128\n",
    "        self.layer1 = Res_2d(1, n_channels, stride=2)\n",
    "        self.layer2 = Res_2d(n_channels, n_channels, stride=2)\n",
    "        self.layer3 = Res_2d(n_channels, n_channels * 2, stride=2)\n",
    "        self.layer4 = Res_2d(n_channels * 2, n_channels * 2, stride=(2, 1))\n",
    "        self.layer5 = Res_2d(n_channels * 2, n_channels * 2, stride=(2, 1))\n",
    "        self.layer6 = Res_2d(n_channels * 2, n_channels * 2, stride=(2, 1))\n",
    "        self.layer7 = Res_2d(n_channels * 2, n_channels * 2, stride=(2, 1))\n",
    "\n",
    "        # Transformer encoder\n",
    "        bert_config = BertConfig(vocab_size=256,\n",
    "                                 hidden_size=256,\n",
    "                                 num_hidden_layers=2,\n",
    "                                 num_attention_heads=8,\n",
    "                                 intermediate_size=1024,\n",
    "                                 hidden_act=\"gelu\",\n",
    "                                 hidden_dropout_prob=0.4,\n",
    "                                 max_position_embeddings=700,\n",
    "                                 attention_probs_dropout_prob=0.5)\n",
    "        self.encoder = BertEncoder(bert_config)\n",
    "        self.pooler = BertPooler(bert_config)\n",
    "        self.vec_cls = self.get_cls(256)\n",
    "\n",
    "        # Dense\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense = nn.Linear(256, n_class)\n",
    "\n",
    "    def get_cls(self, channel):\n",
    "        np.random.seed(0)\n",
    "        single_cls = torch.Tensor(np.random.random((1, channel)))\n",
    "        vec_cls = torch.cat([single_cls for _ in range(64)], dim=0)\n",
    "        vec_cls = vec_cls.unsqueeze(1)\n",
    "        return vec_cls\n",
    "\n",
    "    def append_cls(self, x):\n",
    "        batch, _, _ = x.size()\n",
    "        part_vec_cls = self.vec_cls[:batch].clone()\n",
    "        part_vec_cls = part_vec_cls.to(x.device)\n",
    "        return torch.cat([part_vec_cls, x], dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Spectrogram\n",
    "        x = self.spec(x)\n",
    "        x = self.to_db(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.spec_bn(x)\n",
    "\n",
    "        # CNN\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = x.squeeze(2)\n",
    "\n",
    "        # Get [CLS] token\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.append_cls(x)\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x[-1]\n",
    "        x = self.pooler(x)\n",
    "\n",
    "        # Dense\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107097,
     "status": "ok",
     "timestamp": 1680549293386,
     "user": {
      "displayName": "Yuxiao Liu",
      "userId": "00349421896740538734"
     },
     "user_tz": 240
    },
    "id": "b8HVJCWOT9GP",
    "outputId": "456e25f9-da20-42cc-f539-6184a0204f1c"
   },
   "outputs": [],
   "source": [
    "model_name = 'cnnsa'\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 15\n",
    "model = get_model(model_name, tags)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "print(\"Training and validating model...\")\n",
    "writer = SummaryWriter('runs/{}_{}_{}_{}'.format(model_name, learning_rate, batch_size, len(tags)))\n",
    "logging.basicConfig(filename=\"log/log_{}_{}_{}_{}\".format(model_name, learning_rate, batch_size, len(tags)),\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "best_pre = float('-inf')\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, epoch, criterion, optimizer, train_loader, False)\n",
    "    pre = validate(model, epoch, criterion, val_loader, False)\n",
    "    if pre > best_pre:\n",
    "        print(\"Best avg precision:\", pre)\n",
    "        best_pre = pre\n",
    "        torch.save(model.state_dict(), 'model/{}_best_score_{}_{}.pt'.format(model_name, learning_rate, len(tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline1(tag_file, npy_root, batch_size, isMap, val_loader, tags):\n",
    "    whole_filenames = sorted(glob.glob(os.path.join(npy_root, \"*/*.npy\")))\n",
    "    train_size = int(len(whole_filenames) * 0.8)\n",
    "    filenames = []\n",
    "    random.shuffle(whole_filenames)\n",
    "    train_filenames = whole_filenames[:train_size]\n",
    "    train_ids = []\n",
    "    for filename in train_filenames:\n",
    "        train_ids.append(filename.split('/')[-2] + '/' + filename.split('/')[-1])\n",
    "    if isMap:\n",
    "        f = open('tag_categorize.json')\n",
    "        data = json.load(f)\n",
    "        categorize = {}\n",
    "        for k, v in data.items():\n",
    "            for i in v[1:-1].split(', '):\n",
    "                categorize[i] = k\n",
    "    train_total_tags = []\n",
    "    with open(tag_file) as fp:\n",
    "        reader = csv.reader(fp, delimiter='\\t')\n",
    "        next(reader, None)  # skip header\n",
    "        for row in reader:\n",
    "            if row[3].replace('.mp3', '.npy') not in train_ids:\n",
    "                # if not in train set\n",
    "                continue\n",
    "            if not os.path.exists(os.path.join(npy_root, row[3].replace('.mp3', '.npy'))):\n",
    "                print(os.path.join(npy_root, row[3].replace('.mp3', '.npy')))\n",
    "                continue\n",
    "            tmp = []\n",
    "            for tag in row[5:]:\n",
    "                if isMap:\n",
    "                    tmp.append(categorize[tag.split('---')[-1]])\n",
    "                else:\n",
    "                    tmp.append(tag.split('---')[-1])\n",
    "            train_total_tags += list(set(tmp))\n",
    "\n",
    "    train_dist_tags = collections.Counter(train_total_tags)\n",
    "    print(train_dist_tags)\n",
    "    total = 0\n",
    "    for v in train_dist_tags.values():\n",
    "        total += v\n",
    "    probs = []\n",
    "    for t in tags:\n",
    "        probs.append(train_dist_tags[t]/total)\n",
    "    labels, outputs = [], []\n",
    "    for _, _, _, label in val_loader:  \n",
    "        labels.append(label)\n",
    "        for _ in range(label.size(0)):\n",
    "            outputs.append(probs)\n",
    "    \n",
    "    outputs = torch.Tensor(outputs)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    assert outputs.shape == labels.shape, \"{}, {}\".format(outputs.shape, labels.shape)\n",
    "    # 1. number of correctly predicted tags divided by the total number of tags\n",
    "    prob_classes = []\n",
    "    for i in range(labels.size(0)):\n",
    "        label = labels[i]\n",
    "        k = label.sum()\n",
    "        _, idx = outputs[i].topk(k=k)\n",
    "        predict = torch.zeros_like(outputs[i])\n",
    "        predict[idx] = 1\n",
    "        prob_classes.append(predict)\n",
    "    prob_classes = torch.stack(prob_classes)\n",
    "    matched_1s = torch.mul(prob_classes, labels)\n",
    "    correct_tag_percentage = matched_1s.sum() / labels.sum()\n",
    "\n",
    "    # 2. Auroc\n",
    "    auroc = multilabel_auroc(outputs, labels, num_labels=N_CLASSES, average=\"macro\", thresholds=None).item()\n",
    "\n",
    "    # 3. avg precision\n",
    "    metric = MultilabelPrecision(average='macro', num_labels=N_CLASSES, thresholds=None).to(device)\n",
    "    pre = metric(outputs, labels).item()\n",
    "\n",
    "    print(\"auroc: {}, pre: {}, avg percent: {}\".format(auroc, pre, correct_tag_percentage))\n",
    "baseline1(tag_file, npy_root, batch_size, False, val_loader, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
